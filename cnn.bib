% Encoding: UTF-8

@Book{Goodfellow2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
}

@Article{LeCun2015,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title     = {Deep learning},
  journal   = {Nature},
  year      = {2015},
  volume    = {521},
  pages     = {436 EP -},
  month     = {May},
  day       = {27},
  doi       = {10.1038/nature14539},
  owner     = {lijun},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN -},
  timestamp = {2017-12-21},
  url       = {http://dx.doi.org/10.1038/nature14539},
}

@Article{Hinton2006a,
  author    = {Geoffrey E. Hinton and Simon Osindero and Yee-Whye Teh},
  title     = {A Fast Learning Algorithm for Deep Belief Nets},
  journal   = {Neural Computation},
  year      = {2006},
  volume    = {18},
  number    = {7},
  pages     = {1527-1554},
  note      = {PMID: 16764513},
  abstract  = { We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. },
  doi       = {10.1162/neco.2006.18.7.1527},
  eprint    = {https://doi.org/10.1162/neco.2006.18.7.1527},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://doi.org/10.1162/neco.2006.18.7.1527

},
}

@Article{Hinton2006b,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  journal   = {Science},
  year      = {2006},
  volume    = {313},
  number    = {5786},
  pages     = {504--507},
  issn      = {0036-8075},
  abstract  = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  doi       = {10.1126/science.1127647},
  eprint    = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
  owner     = {lijun},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2017-12-21},
  url       = {http://science.sciencemag.org/content/313/5786/504},
}

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{Simonyan2014,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.1556},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  biburl        = {http://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  eprint        = {1409.1556},
  owner         = {lijun},
  timestamp     = {2017-12-21},
  url           = {http://arxiv.org/abs/1409.1556},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going Deeper With Convolutions},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
}

@inproceedings{Szegedy2017,
  title={Inception-v4, inception-resnet and the impact of residual connections on learning.},
  author={Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A},
  booktitle={AAAI},
  volume={4},
  pages={12},
  year={2017}
}


@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  month     = {June},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
}

@Article{Hinton2012,
  author    = {G. Hinton and L. Deng and D. Yu and G. E. Dahl and A. r. Mohamed and N. Jaitly and A. Senior and V. Vanhoucke and P. Nguyen and T. N. Sainath and B. Kingsbury},
  title     = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  journal   = {IEEE Signal Processing Magazine},
  year      = {2012},
  volume    = {29},
  number    = {6},
  pages     = {82-97},
  month     = {Nov},
  issn      = {1053-5888},
  doi       = {10.1109/MSP.2012.2205597},
  keywords  = {Gaussian processes;feedforward neural nets;hidden Markov models;speech recognition;Gaussian mixture models;HMM states;acoustic modeling;deep neural networks;feed-forward neural network;hidden Markov models;posterior probabilities;speech recognition;temporal variability;Acoustics;Automatic speech recognition;Data models;Gaussian processes;Hidden Markov models;Neural networks;Speech recognition;Training},
}

@InProceedings{Deng2009,
  author    = {J. Deng and W. Dong and R. Socher and L. J. Li and Kai Li and Li Fei-Fei},
  title     = {{ImageNet:} A large-scale hierarchical image database},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2009},
  pages     = {248-255},
  month     = {June},
  doi       = {10.1109/CVPR.2009.5206848},
  issn      = {1063-6919},
  keywords  = {Internet;computer vision;image resolution;image retrieval;multimedia computing;ontologies (artificial intelligence);trees (mathematics);very large databases;visual databases;ImageNet database;Internet;computer vision;image resolution;image retrieval;large-scale hierarchical image database;large-scale ontology;multimedia data;subtree;wordNet structure;Explosions;Image databases;Image retrieval;Information retrieval;Internet;Large-scale systems;Multimedia databases;Ontologies;Robustness;Spine},
}

@InProceedings{Hinton1986,
  author       = {Hinton, Geoffrey E},
  title        = {Learning distributed representations of concepts},
  booktitle    = {Proceedings of the eighth annual conference of the cognitive science society},
  year         = {1986},
  volume       = {1},
  pages        = {12},
  organization = {Amherst, MA},
}

@Article{ruder2016overview,
  author  = {Ruder, Sebastian},
  title   = {An overview of gradient descent optimization algorithms},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2016},
}

@Article{LeCun1989,
  author    = {LeCun, Yann and others},
  title     = {Generalization and network design strategies},
  journal   = {Connectionism in perspective},
  year      = {1989},
  pages     = {143--155},
  publisher = {Zurich, Switzerland: Elsevier},
}

@InProceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  title     = {Rectified linear units improve restricted boltzmann machines},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  year      = {2010},
  pages     = {807--814},
}

@InProceedings{Zhou1988,
  author    = {Y. T. Zhou and R. Chellappa},
  title     = {Computation of optical flow using a neural network},
  booktitle = {IEEE 1988 International Conference on Neural Networks},
  year      = {1988},
  pages     = {71-78 vol.2},
  month     = {July},
  doi       = {10.1109/ICNN.1988.23914},
  keywords  = {computerised picture processing;neural nets;2-D polynomial;computerised picture processing;neural nets;neural network;optical flow;smooth continuous image-intensity function;synthetic image sequences;Image processing;Neural networks},
}

@InProceedings{Ioffe2015,
  author    = {Sergey Ioffe and Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  year      = {2015},
  editor    = {Francis Bach and David Blei},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  pages     = {448--456},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  file      = {ioffe15.pdf:http\://proceedings.mlr.press/v37/ioffe15.pdf:PDF},
  url       = {http://proceedings.mlr.press/v37/ioffe15.html},
}

@InCollection{Desjardins2015,
  author    = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
  title     = {Natural Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {2071--2079},
  url       = {http://papers.nips.cc/paper/5953-natural-neural-networks.pdf},
}

@Article{Duchi2011,
  author    = {Duchi, John and Hazan, Elad and Singer, Yoram},
  title     = {Adaptive subgradient methods for online learning and stochastic optimization},
  journal   = {Journal of Machine Learning Research},
  year      = {2011},
  volume    = {12},
  number    = {Jul},
  pages     = {2121--2159},
}

@Article{Tieleman2012,
  author  = {Tieleman, Tijmen and Hinton, Geoffrey},
  title   = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  journal = {COURSERA: Neural networks for machine learning},
  year    = {2012},
  volume  = {4},
  number  = {2},
  pages   = {26--31},
}

@Article{Kingma2014,
  author    = {Kingma, Diederik and Ba, Jimmy},
  title     = {Adam: A method for stochastic optimization},
  journal   = {arXiv preprint arXiv:1412.6980},
  year      = {2014},
}

@techreport{Rosenblatt1961,
  title={Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, Frank},
  year={1961},
  institution={Cornell Aeronautical Lab Inc., Buffalo NY},
}

@article{Molchanov2016,
  author    = {Pavlo Molchanov and
               Stephen Tyree and
               Tero Karras and
               Timo Aila and
               Jan Kautz},
  title     = {Pruning Convolutional Neural Networks for Resource Efficient Transfer
               Learning},
  journal   = {CoRR},
  volume    = {abs/1611.06440},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.06440},
  archivePrefix = {arXiv},
  eprint    = {1611.06440},
}

@InProceedings{Kim2016,
author = {Kim, Jiwon and Kwon Lee, Jung and Mu Lee, Kyoung},
title = {Deeply-Recursive Convolutional Network for Image Super-Resolution},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}


@InProceedings{Long2015,
author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
title = {Fully Convolutional Networks for Semantic Segmentation},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@Article{Bell2007,
 author = {Bell, Robert M. and Koren, Yehuda},
 title = {Lessons from the Netflix Prize Challenge},
 journal = {SIGKDD Explor. Newsl.},
 issue_date = {December 2007},
 volume = {9},
 number = {2},
 month = dec,
 year = {2007},
 issn = {1931-0145},
 pages = {75--79},
 numpages = {5},
 url = {http://doi.acm.org/10.1145/1345448.1345465},
 doi = {10.1145/1345448.1345465},
 acmid = {1345465},
 publisher = {ACM},
 address = {New York, NY, USA},
}


@article{Robbins1951,
author = "Robbins, Herbert and Monro, Sutton",
doi = "10.1214/aoms/1177729586",
fjournal = "The Annals of Mathematical Statistics",
journal = "Ann. Math. Statist.",
month = "09",
number = "3",
pages = "400--407",
publisher = "The Institute of Mathematical Statistics",
title = "A Stochastic Approximation Method",
url = "https://doi.org/10.1214/aoms/1177729586",
volume = "22",
year = "1951"
}


@inproceedings{Jacob2018,
title={Quantization and training of neural networks for efficient
integer-arithmetic-only inference},
author={Jacob, Benoit and Kligys, Skirmantas and Chen, Bo and Zhu,
Menglong and Tang, Matthew and Howard, Andrew and Adam, Hartwig and
Kalenichenko, Dmitry},
booktitle={Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
pages={2704--2713},
year={2018}
}

@Article{Krishnamoorthi2018,
  author  = {lRuder, Sebastian, Raghuraman},
  title   = {An overview of gradient descent optimization algorithms},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2018},
}

@article{Howard2017,
title={Mobilenets: Efficient convolutional neural networks for mobile vision applications},
author={Howard, Andrew G and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
journal={arXiv preprint arXiv:1704.04861},
year={2017}
}

@InProceedings{LiangHu2015,
author = {Liang, Ming and Hu, Xiaolin},
title = {Recurrent Convolutional Neural Network for Object Recognition},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@Comment{jabref-meta: databaseType:bibtex;}
