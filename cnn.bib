% Encoding: UTF-8

@Book{Goodfellow2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
  owner     = {lijun},
  timestamp = {2017-12-21},
}

@Article{LeCun2015,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title     = {Deep learning},
  journal   = {Nature},
  year      = {2015},
  volume    = {521},
  pages     = {436 EP -},
  month     = {May},
  day       = {27},
  doi       = {10.1038/nature14539},
  owner     = {lijun},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN -},
  timestamp = {2017-12-21},
  url       = {http://dx.doi.org/10.1038/nature14539},
}

@Article{Hinton2006a,
  author    = {Geoffrey E. Hinton and Simon Osindero and Yee-Whye Teh},
  title     = {A Fast Learning Algorithm for Deep Belief Nets},
  journal   = {Neural Computation},
  year      = {2006},
  volume    = {18},
  number    = {7},
  pages     = {1527-1554},
  note      = {PMID: 16764513},
  abstract  = { We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. },
  doi       = {10.1162/neco.2006.18.7.1527},
  eprint    = {https://doi.org/10.1162/neco.2006.18.7.1527},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://doi.org/10.1162/neco.2006.18.7.1527

},
}

@Article{Hinton2006b,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  journal   = {Science},
  year      = {2006},
  volume    = {313},
  number    = {5786},
  pages     = {504--507},
  issn      = {0036-8075},
  abstract  = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  doi       = {10.1126/science.1127647},
  eprint    = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
  owner     = {lijun},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2017-12-21},
  url       = {http://science.sciencemag.org/content/313/5786/504},
}

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{Simonyan2014,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.1556},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  biburl        = {http://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  eprint        = {1409.1556},
  owner         = {lijun},
  timestamp     = {2017-12-21},
  url           = {http://arxiv.org/abs/1409.1556},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going Deeper With Convolutions},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
}

@Article{Hinton2012,
  author    = {G. Hinton and L. Deng and D. Yu and G. E. Dahl and A. r. Mohamed and N. Jaitly and A. Senior and V. Vanhoucke and P. Nguyen and T. N. Sainath and B. Kingsbury},
  title     = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  journal   = {IEEE Signal Processing Magazine},
  year      = {2012},
  volume    = {29},
  number    = {6},
  pages     = {82-97},
  month     = {Nov},
  issn      = {1053-5888},
  abstract  = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few (<;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  doi       = {10.1109/MSP.2012.2205597},
  keywords  = {Gaussian processes;feedforward neural nets;hidden Markov models;speech recognition;Gaussian mixture models;HMM states;acoustic modeling;deep neural networks;feed-forward neural network;hidden Markov models;posterior probabilities;speech recognition;temporal variability;Acoustics;Automatic speech recognition;Data models;Gaussian processes;Hidden Markov models;Neural networks;Speech recognition;Training},
  owner     = {lijun},
  timestamp = {2017-12-27},
}

@InProceedings{Deng2009,
  author    = {J. Deng and W. Dong and R. Socher and L. J. Li and Kai Li and Li Fei-Fei},
  title     = {ImageNet: A large-scale hierarchical image database},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2009},
  pages     = {248-255},
  month     = {June},
  abstract  = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few (<;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  doi       = {10.1109/CVPR.2009.5206848},
  issn      = {1063-6919},
  keywords  = {Internet;computer vision;image resolution;image retrieval;multimedia computing;ontologies (artificial intelligence);trees (mathematics);very large databases;visual databases;ImageNet database;Internet;computer vision;image resolution;image retrieval;large-scale hierarchical image database;large-scale ontology;multimedia data;subtree;wordNet structure;Explosions;Image databases;Image retrieval;Information retrieval;Internet;Large-scale systems;Multimedia databases;Ontologies;Robustness;Spine},
  owner     = {lijun},
  timestamp = {2017-12-27},
}

@InProceedings{Hinton1986,
  author       = {Hinton, Geoffrey E},
  title        = {Learning distributed representations of concepts},
  booktitle    = {Proceedings of the eighth annual conference of the cognitive science society},
  year         = {1986},
  volume       = {1},
  pages        = {12},
  organization = {Amherst, MA},
  owner        = {lijun},
  timestamp    = {2017-12-27},
}

@Article{ruder2016overview,
  author  = {Ruder, Sebastian},
  title   = {An overview of gradient descent optimization algorithms},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2016},
}

@InProceedings{Chen2016,
  author    = {Tianqi Chen and Ian Goodfellow and Jonathon Shlens},
  title     = {Net2Net: Accelerating Learning via Knowledge Transfer},
  booktitle = {International Conference on Learning Representations},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.05641},
}

@InProceedings{Wei2016,
  author    = {Tao Wei and Changhu Wang and Yong Rui and Chang Wen Chen},
  title     = {Network Morphism},
  booktitle = {Proceedings of The 33rd International Conference on Machine Learning},
  year      = {2016},
  editor    = {Maria Florina Balcan and Kilian Q. Weinberger},
  volume    = {48},
  series    = {Proceedings of Machine Learning Research},
  pages     = {564--572},
  address   = {New York, New York, USA},
  month     = {20--22 Jun},
  publisher = {PMLR},
  abstract  = {We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous non-linear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme.},
  pdf       = {http://proceedings.mlr.press/v48/wei16.pdf},
  url       = {http://proceedings.mlr.press/v48/wei16.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
