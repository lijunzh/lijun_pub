% Encoding: UTF-8

@Book{Goodfellow2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
  owner     = {lijun},
  timestamp = {2017-12-21},
}

@Article{LeCun2015,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title     = {Deep learning},
  journal   = {Nature},
  year      = {2015},
  volume    = {521},
  pages     = {436 EP -},
  month     = {May},
  day       = {27},
  doi       = {10.1038/nature14539},
  owner     = {lijun},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN -},
  timestamp = {2017-12-21},
  url       = {http://dx.doi.org/10.1038/nature14539},
}

@Article{Hinton2006a,
  author    = {Geoffrey E. Hinton and Simon Osindero and Yee-Whye Teh},
  title     = {A Fast Learning Algorithm for Deep Belief Nets},
  journal   = {Neural Computation},
  year      = {2006},
  volume    = {18},
  number    = {7},
  pages     = {1527-1554},
  note      = {PMID: 16764513},
  abstract  = { We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. },
  doi       = {10.1162/neco.2006.18.7.1527},
  eprint    = {https://doi.org/10.1162/neco.2006.18.7.1527},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://doi.org/10.1162/neco.2006.18.7.1527

},
}

@Article{Hinton2006b,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  journal   = {Science},
  year      = {2006},
  volume    = {313},
  number    = {5786},
  pages     = {504--507},
  issn      = {0036-8075},
  abstract  = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  doi       = {10.1126/science.1127647},
  eprint    = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
  owner     = {lijun},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2017-12-21},
  url       = {http://science.sciencemag.org/content/313/5786/504},
}

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{Simonyan2014,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.1556},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  biburl        = {http://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  eprint        = {1409.1556},
  owner         = {lijun},
  timestamp     = {2017-12-21},
  url           = {http://arxiv.org/abs/1409.1556},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going Deeper With Convolutions},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
}

@Article{Hinton2012,
  author    = {G. Hinton and L. Deng and D. Yu and G. E. Dahl and A. r. Mohamed and N. Jaitly and A. Senior and V. Vanhoucke and P. Nguyen and T. N. Sainath and B. Kingsbury},
  title     = {Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups},
  journal   = {IEEE Signal Processing Magazine},
  year      = {2012},
  volume    = {29},
  number    = {6},
  pages     = {82-97},
  month     = {Nov},
  issn      = {1053-5888},
  abstract  = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few (<;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  doi       = {10.1109/MSP.2012.2205597},
  keywords  = {Gaussian processes;feedforward neural nets;hidden Markov models;speech recognition;Gaussian mixture models;HMM states;acoustic modeling;deep neural networks;feed-forward neural network;hidden Markov models;posterior probabilities;speech recognition;temporal variability;Acoustics;Automatic speech recognition;Data models;Gaussian processes;Hidden Markov models;Neural networks;Speech recognition;Training},
  owner     = {lijun},
  timestamp = {2017-12-27},
}

@InProceedings{Deng2009,
  author    = {J. Deng and W. Dong and R. Socher and L. J. Li and Kai Li and Li Fei-Fei},
  title     = {{ImageNet:} A large-scale hierarchical image database},
  booktitle = {2009 IEEE Conference on Computer Vision and Pattern Recognition},
  year      = {2009},
  pages     = {248-255},
  month     = {June},
  abstract  = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few (<;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  doi       = {10.1109/CVPR.2009.5206848},
  issn      = {1063-6919},
  keywords  = {Internet;computer vision;image resolution;image retrieval;multimedia computing;ontologies (artificial intelligence);trees (mathematics);very large databases;visual databases;ImageNet database;Internet;computer vision;image resolution;image retrieval;large-scale hierarchical image database;large-scale ontology;multimedia data;subtree;wordNet structure;Explosions;Image databases;Image retrieval;Information retrieval;Internet;Large-scale systems;Multimedia databases;Ontologies;Robustness;Spine},
  owner     = {lijun},
  timestamp = {2017-12-27},
}

@InProceedings{Hinton1986,
  author       = {Hinton, Geoffrey E},
  title        = {Learning distributed representations of concepts},
  booktitle    = {Proceedings of the eighth annual conference of the cognitive science society},
  year         = {1986},
  volume       = {1},
  pages        = {12},
  organization = {Amherst, MA},
  owner        = {lijun},
  timestamp    = {2017-12-27},
}

@Article{ruder2016overview,
  author  = {Ruder, Sebastian},
  title   = {An overview of gradient descent optimization algorithms},
  journal = {arXiv preprint arXiv:1609.04747},
  year    = {2016},
}

@Article{LeCun1989,
  author    = {LeCun, Yann and others},
  title     = {Generalization and network design strategies},
  journal   = {Connectionism in perspective},
  year      = {1989},
  pages     = {143--155},
  publisher = {Zurich, Switzerland: Elsevier},
}

@InProceedings{Nair2010,
  author    = {Nair, Vinod and Hinton, Geoffrey E},
  title     = {Rectified linear units improve restricted boltzmann machines},
  booktitle = {Proceedings of the 27th international conference on machine learning (ICML-10)},
  year      = {2010},
  pages     = {807--814},
  owner     = {lijun},
  timestamp = {2017-12-30},
}

@InProceedings{Zhou1988,
  author    = {Y. T. Zhou and R. Chellappa},
  title     = {Computation of optical flow using a neural network},
  booktitle = {IEEE 1988 International Conference on Neural Networks},
  year      = {1988},
  pages     = {71-78 vol.2},
  month     = {July},
  abstract  = {Summary form only given. Strong light-matter coupling has been recently successfully explored in the GHz and THz [1] range with on-chip platforms. New and intriguing quantum optical phenomena have been predicted in the ultrastrong coupling regime [2], when the coupling strength Ω becomes comparable to the unperturbed frequency of the system ω. We recently proposed a new experimental platform where we couple the inter-Landau level transition of an high-mobility 2DEG to the highly subwavelength photonic mode of an LC meta-atom [3] showing very large Ω/ωc = 0.87. Our system benefits from the collective enhancement of the light-matter coupling which comes from the scaling of the coupling Ω ∝ √n, were n is the number of optically active electrons. In our previous experiments [3] and in literature [4] this number varies from 104-103 electrons per meta-atom. We now engineer a new cavity, resonant at 290 GHz, with an extremely reduced effective mode surface Seff = 4 × 10-14 m2 (FE simulations, CST), yielding large field enhancements above 1500 and allowing to enter the few (<;100) electron regime. It consist of a complementary metasurface with two very sharp metallic tips separated by a 60 nm gap (Fig.1(a, b)) on top of a single triangular quantum well. THz-TDS transmission experiments as a function of the applied magnetic field reveal strong anticrossing of the cavity mode with linear cyclotron dispersion. Measurements for arrays of only 12 cavities are reported in Fig.1(c). On the top horizontal axis we report the number of electrons occupying the topmost Landau level as a function of the magnetic field. At the anticrossing field of B=0.73 T we measure approximately 60 electrons ultra strongly coupled (Ω/ω- ||},
  doi       = {10.1109/ICNN.1988.23914},
  keywords  = {computerised picture processing;neural nets;2-D polynomial;computerised picture processing;neural nets;neural network;optical flow;smooth continuous image-intensity function;synthetic image sequences;Image processing;Neural networks},
  owner     = {lijun},
  timestamp = {2017-12-30},
}

@InProceedings{Ioffe2015,
  author    = {Sergey Ioffe and Christian Szegedy},
  title     = {Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  year      = {2015},
  editor    = {Francis Bach and David Blei},
  volume    = {37},
  series    = {Proceedings of Machine Learning Research},
  pages     = {448--456},
  address   = {Lille, France},
  month     = {07--09 Jul},
  publisher = {PMLR},
  abstract  = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer’s inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a stateof-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters.},
  file      = {ioffe15.pdf:http\://proceedings.mlr.press/v37/ioffe15.pdf:PDF},
  owner     = {lijun},
  timestamp = {2017-12-30},
  url       = {http://proceedings.mlr.press/v37/ioffe15.html},
}

@InCollection{Desjardins2015,
  author    = {Desjardins, Guillaume and Simonyan, Karen and Pascanu, Razvan and Kavukcuoglu, Koray},
  title     = {Natural Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 28},
  publisher = {Curran Associates, Inc.},
  year      = {2015},
  editor    = {C. Cortes and N. D. Lawrence and D. D. Lee and M. Sugiyama and R. Garnett},
  pages     = {2071--2079},
  url       = {http://papers.nips.cc/paper/5953-natural-neural-networks.pdf},
}

@Article{Duchi2011,
  author    = {Duchi, John and Hazan, Elad and Singer, Yoram},
  title     = {Adaptive subgradient methods for online learning and stochastic optimization},
  journal   = {Journal of Machine Learning Research},
  year      = {2011},
  volume    = {12},
  number    = {Jul},
  pages     = {2121--2159},
  abstract  = {We present a new family of subgradient methods that dynamically incorporate knowledge of the geometry of the data observed in earlier iterations to perform more informative gradient-based learning. Metaphorically, the adaptation allows us to find needles in haystacks in the form of very predictive but rarely seen features. Our paradigm stems from recent advances in stochastic optimization and online learning which employ proximal functions to control the gradient steps of the algorithm. We describe and analyze an apparatus for adaptively modifying the proximal function, which significantly simplifies setting a learning rate and results in regret guarantees that are provably as good as the best proximal function that can be chosen in hindsight. We give several efficient algorithms for empirical risk minimization problems with common and important regularization functions and domain constraints. We experimentally study our theoretical analysis and show that adaptive subgradient methods outperform state-of-the-art, yet non-adaptive, subgradient algorithms. },
  owner     = {lijun},
  timestamp = {2017-12-30},
}

@Article{Tieleman2012,
  author  = {Tieleman, Tijmen and Hinton, Geoffrey},
  title   = {Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  journal = {COURSERA: Neural networks for machine learning},
  year    = {2012},
  volume  = {4},
  number  = {2},
  pages   = {26--31},
}

@Article{Kingma2014,
  author    = {Kingma, Diederik and Ba, Jimmy},
  title     = {Adam: A method for stochastic optimization},
  journal   = {arXiv preprint arXiv:1412.6980},
  year      = {2014},
  owner     = {lijun},
  timestamp = {2017-12-30},
}

@techreport{Rosenblatt1961,
  title={Principles of neurodynamics. perceptrons and the theory of brain mechanisms},
  author={Rosenblatt, Frank},
  year={1961},
  institution={Cornell Aeronautical Lab Inc., Buffalo NY},
  owner     = {lijun},
  timestamp = {2018-1-24},
}

@article{Paszke2017,
  title={Automatic differentiation in {PyTorch}},
  author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
  year={2017}
}

@article{Beyreuther2010obspy,
  title={{ObsPy:} A Python toolbox for seismology},
  author={Beyreuther, Moritz and Barsch, Robert and Krischer, Lion and Megies, Tobias and Behr, Yannik and Wassermann, Joachim},
  journal={Seismological Research Letters},
  volume={81},
  number={3},
  pages={530--533},
  year={2010},
  publisher={Seismological Society of America}
}

@article{Molchanov2016,
  author    = {Pavlo Molchanov and
               Stephen Tyree and
               Tero Karras and
               Timo Aila and
               Jan Kautz},
  title     = {Pruning Convolutional Neural Networks for Resource Efficient Transfer
               Learning},
  journal   = {CoRR},
  volume    = {abs/1611.06440},
  year      = {2016},
  url       = {http://arxiv.org/abs/1611.06440},
  archivePrefix = {arXiv},
  eprint    = {1611.06440},
  timestamp = {Mon, 13 Aug 2018 16:46:56 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/MolchanovTKAK16},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}



@Comment{jabref-meta: databaseType:bibtex;}
