% Encoding: UTF-8

@Book{Goodfellow2016,
  title     = {Deep Learning},
  publisher = {MIT Press},
  year      = {2016},
  author    = {Ian Goodfellow and Yoshua Bengio and Aaron Courville},
  note      = {\url{http://www.deeplearningbook.org}},
  owner     = {lijun},
  timestamp = {2017-12-21},
}

@Article{LeCun2015,
  author    = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  title     = {Deep learning},
  journal   = {Nature},
  year      = {2015},
  volume    = {521},
  pages     = {436 EP -},
  month     = {May},
  day       = {27},
  owner     = {lijun},
  publisher = {Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved. SN -},
  timestamp = {2017-12-21},
  url       = {http://dx.doi.org/10.1038/nature14539},
}

@Article{Hinton2006a,
  author    = {Geoffrey E. Hinton and Simon Osindero and Yee-Whye Teh},
  title     = {A Fast Learning Algorithm for Deep Belief Nets},
  journal   = {Neural Computation},
  year      = {2006},
  volume    = {18},
  number    = {7},
  pages     = {1527-1554},
  note      = {PMID: 16764513},
  abstract  = { We show how to use “complementary priors” to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind. },
  doi       = {10.1162/neco.2006.18.7.1527},
  eprint    = {https://doi.org/10.1162/neco.2006.18.7.1527},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = { 
        https://doi.org/10.1162/neco.2006.18.7.1527
    
},
}

@Article{Hinton2006b,
  author    = {Hinton, G. E. and Salakhutdinov, R. R.},
  title     = {Reducing the Dimensionality of Data with Neural Networks},
  journal   = {Science},
  year      = {2006},
  volume    = {313},
  number    = {5786},
  pages     = {504--507},
  issn      = {0036-8075},
  abstract  = {High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such {\textquotedblleft}autoencoder{\textquotedblright} networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep autoencoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.},
  doi       = {10.1126/science.1127647},
  eprint    = {http://science.sciencemag.org/content/313/5786/504.full.pdf},
  owner     = {lijun},
  publisher = {American Association for the Advancement of Science},
  timestamp = {2017-12-21},
  url       = {http://science.sciencemag.org/content/313/5786/504},
}

@InCollection{Krizhevsky2012,
  author    = {Alex Krizhevsky and Sutskever, Ilya and Hinton, Geoffrey E},
  title     = {ImageNet Classification with Deep Convolutional Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems 25},
  publisher = {Curran Associates, Inc.},
  year      = {2012},
  editor    = {F. Pereira and C. J. C. Burges and L. Bottou and K. Q. Weinberger},
  pages     = {1097--1105},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
}

@Article{Simonyan2014,
  author        = {Karen Simonyan and Andrew Zisserman},
  title         = {Very Deep Convolutional Networks for Large-Scale Image Recognition},
  journal       = {CoRR},
  year          = {2014},
  volume        = {abs/1409.1556},
  archiveprefix = {arXiv},
  bibsource     = {dblp computer science bibliography, http://dblp.org},
  biburl        = {http://dblp.org/rec/bib/journals/corr/SimonyanZ14a},
  eprint        = {1409.1556},
  owner         = {lijun},
  timestamp     = {2017-12-21},
  url           = {http://arxiv.org/abs/1409.1556},
}

@InProceedings{Szegedy2015,
  author    = {Szegedy, Christian and Liu, Wei and Jia, Yangqing and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  title     = {Going Deeper With Convolutions},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2015},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2015/html/Szegedy_Going_Deeper_With_2015_CVPR_paper.html},
}

@InProceedings{He2016,
  author    = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  title     = {Deep Residual Learning for Image Recognition},
  booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  year      = {2016},
  month     = {June},
  owner     = {lijun},
  timestamp = {2017-12-21},
  url       = {https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html},
}

@Comment{jabref-meta: databaseType:bibtex;}
